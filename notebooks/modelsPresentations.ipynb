{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leaf Classification Kaggel Project\n",
    "\n",
    "## Implemented models :\n",
    "\n",
    "* Logistic Regression\n",
    "* Support Vector Machine\n",
    "* Neurenal network\n",
    "* AdaBoost\n",
    "* Random Forrest\n",
    "* Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    " Imporation des mod√®les\n",
    "'''\n",
    "import os,sys,inspect\n",
    "\n",
    "current_dir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "relative_path = parent_dir + \"/src\"\n",
    "sys.path.insert(0, relative_path)\n",
    "\n",
    "import models.logistical_regression_model as lr\n",
    "import models.adaBoost_model as ab\n",
    "import models.randomForrest_model as rf\n",
    "import models.gaussianNaiveBayes_model as gnb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Model : Logistic regression classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = lr.LogisticalRegressionModel()\n",
    "\n",
    "data_to_learn_with = 'features'\n",
    "\n",
    "load_model = True    # If true then we will load a model that is already trained, else we will train a model\n",
    "\n",
    "if data_to_learn_with == 'features':\n",
    "    if load_model:\n",
    "        log_reg.load_model_features()\n",
    "    else:\n",
    "        log_reg.train_model_features()\n",
    "\n",
    "    training_features_loss = log_reg.calculate_training_loss()\n",
    "    print(\"training_loss_model_features: \",training_features_loss)\n",
    "    \n",
    "elif data_to_learn_with == 'images':\n",
    "    if load_model:\n",
    "        log_reg.load_model_images()\n",
    "    else:\n",
    "        log_reg.train_model_images()\n",
    "\n",
    "    training_loss_images = log_reg.calculate_training_loss_images()\n",
    "    print(\"training_loss_model_images: \",training_loss_images)\n",
    "    \n",
    "elif data_to_learn_with == 'features_images':\n",
    "    if load_model: \n",
    "        log_reg.load_model_features_images()\n",
    "    else:\n",
    "        log_reg.train_model_images_features()\n",
    "        \n",
    "    training_loss_images_features = log_reg.calculate_training_loss_features_images()\n",
    "    print(\"training_loss_model_images_features: \",training_loss_images_features)\n",
    "\n",
    "elif data_to_learn_with == 'pca_features':\n",
    "    if load_model:\n",
    "        number_of_components_model_to_load = 167\n",
    "        log_reg.load_model_pca(number_of_components_model_to_load)\n",
    "    else:\n",
    "        train_with_cross_validation = False\n",
    "        if train_with_cross_validation:\n",
    "            cross_validation_by_minimizing_all_features_loss = True\n",
    "            if cross_validation_by_minimizing_all_features_loss:  # If True then the model will find the number of components that minimizes the error of all data and will be trained on all the data\n",
    "                log_reg.train_model_pca_cross_validation()        #The model finds the number of PCA components that minimizes the validation error\n",
    "            else:                                                 #If False then the model will find the number of components that minimizes only the error of the validation features (80)\n",
    "                log_reg.train_model_pca_cross_validation('data_splited')   #The model finds the number of PCA components that minimizes the validation data (20% of all data) and will be trained all 80% of the training data\n",
    "\n",
    "        else:\n",
    "            number_of_components_to_train_with = 167              #It's a number between 1-192\n",
    "            log_reg.train_model_pca(num_comp=number_of_components_to_train_with)\n",
    "    \n",
    "    training_loss_features_pca = log_reg.calculate_training_loss_pca_data()\n",
    "    print(\"training_loss_model_PCA: \",training_loss_features_pca)\n",
    "\n",
    "else:\n",
    "    print(\"ERROR, data_to_learn_with must be : 'features', 'images', 'features_images' or 'pca_features' \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second Model : AdaBoost classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss_model_PCA:  4.59503113734191\n"
     ]
    }
   ],
   "source": [
    "ada_boost = ab.AdaBoostModel()\n",
    "\n",
    "data_to_learn_with = 'pca_features'\n",
    "\n",
    "load_model = True    # If true then we will load a model that is already trained, else we will train a model\n",
    "train_with_cross_validation = False #Cross Validation upon the number of estimators\n",
    "n_estimators = 1000\n",
    "\n",
    "if data_to_learn_with == 'features':\n",
    "    if load_model:\n",
    "        ada_boost.load_model_features(n_estimators)   #Load the model that has been trained with number of estimators = n_estimators\n",
    "    else:\n",
    "        if train_with_cross_validation:\n",
    "            ada_boost.train_model_features_cross_validation()\n",
    "        else:\n",
    "            ada_boost.train_model_features(n_estimators)\n",
    "\n",
    "    training_features_loss = ada_boost.calculate_training_loss()\n",
    "    print(\"training_loss_model_features: \",training_features_loss)\n",
    "    \n",
    "elif data_to_learn_with == 'images':\n",
    "    if load_model:\n",
    "        ada_boost.load_model_images(n_estimators)   #Load the model that has been trained with number of estimators = n_estimators\n",
    "    else:\n",
    "        if train_with_cross_validation:\n",
    "            ada_boost.train_model_images_cross_validation()\n",
    "        else:\n",
    "            ada_boost.train_model_images(n_estimators)\n",
    "\n",
    "    training_loss_images = ada_boost.calculate_training_loss_images()\n",
    "    print(\"training_loss_model_images: \",training_loss_images)\n",
    "    \n",
    "elif data_to_learn_with == 'features_images':\n",
    "    if load_model:\n",
    "        ada_boost.load_model_features_images(n_estimators)   #Load the model that has been trained with number of estimators = n_estimators\n",
    "    else:\n",
    "        if train_with_cross_validation:\n",
    "            ada_boost.train_model_features_images_cross_validation()\n",
    "        else:\n",
    "            ada_boost.train_model_images_features(n_estimators)\n",
    "        \n",
    "    training_loss_images_features = ada_boost.calculate_training_loss_features_images()\n",
    "    print(\"training_loss_model_images_features: \",training_loss_images_features)\n",
    "\n",
    "elif data_to_learn_with == 'pca_features':\n",
    "    if load_model:\n",
    "        number_of_components_model_to_load = 167\n",
    "        ada_boost.load_model_pca(number_of_components_model_to_load, n_estimators)\n",
    "    else:\n",
    "        if train_with_cross_validation:\n",
    "            cross_validation_by_minimizing_all_features_loss = True\n",
    "            if cross_validation_by_minimizing_all_features_loss:  # If True then the model will find the number of components that minimizes the error of all data and will be trained on all the data\n",
    "                ada_boost.train_model_pca_cross_validation()        #The model finds the number of PCA components that minimizes the validation error\n",
    "            else:                                                 #If False then the model will find the number of components that minimizes only the error of the validation features (80)\n",
    "                ada_boost.train_model_pca_cross_validation('data_splited')   #The model finds the number of PCA components that minimizes the validation data (20% of all data) and will be trained all 80% of the training data\n",
    "\n",
    "        else:\n",
    "            number_of_components_to_train_with = 167              #It's a number between 1-192\n",
    "            ada_boost.train_model_pca(num_comp=number_of_components_to_train_with, n_estimators=n_estimators)\n",
    "    \n",
    "    training_loss_features_pca = ada_boost.calculate_training_loss_pca_data()\n",
    "    print(\"training_loss_model_PCA: \",training_loss_features_pca)\n",
    "else:\n",
    "    print(\"ERROR, data_to_learn_with must be : 'features', 'images', 'features_images' or 'pca_features' \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Third Model : Random Forrest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss_model_PCA:  0.29619687768461145\n"
     ]
    }
   ],
   "source": [
    "rand_forrest = rf.AdaBoostModel()\n",
    "\n",
    "data_to_learn_with = 'pca_features'\n",
    "\n",
    "load_model = True    # If true then we will load a model that is already trained, else we will train a model\n",
    "train_with_cross_validation = False #Cross Validation upon the number of estimators\n",
    "n_estimators = 300\n",
    "\n",
    "if data_to_learn_with == 'features':\n",
    "    if load_model:\n",
    "        rand_forrest.load_model_features(n_estimators)   #Load the model that has been trained with number of estimators = n_estimators\n",
    "    else:\n",
    "        if train_with_cross_validation:\n",
    "            rand_forrest.train_model_features_cross_validation()\n",
    "        else:\n",
    "            rand_forrest.train_model_features(n_estimators)\n",
    "\n",
    "    training_features_loss = rand_forrest.calculate_training_loss()\n",
    "    print(\"training_loss_model_features: \",training_features_loss)\n",
    "    \n",
    "elif data_to_learn_with == 'images':\n",
    "    if load_model:\n",
    "        rand_forrest.load_model_images(n_estimators)   #Load the model that has been trained with number of estimators = n_estimators\n",
    "    else:\n",
    "        if train_with_cross_validation:\n",
    "            rand_forrest.train_model_images_cross_validation()\n",
    "        else:\n",
    "            rand_forrest.train_model_images(n_estimators)\n",
    "\n",
    "    training_loss_images = rand_forrest.calculate_training_loss_images()\n",
    "    print(\"training_loss_model_images: \",training_loss_images)\n",
    "    \n",
    "elif data_to_learn_with == 'features_images':\n",
    "    if load_model:\n",
    "        rand_forrest.load_model_features_images(n_estimators)   #Load the model that has been trained with number of estimators = n_estimators\n",
    "    else:\n",
    "        if train_with_cross_validation:\n",
    "            rand_forrest.train_model_features_images_cross_validation()\n",
    "        else:\n",
    "            rand_forrest.train_model_images_features(n_estimators)\n",
    "        \n",
    "    training_loss_images_features = rand_forrest.calculate_training_loss_features_images()\n",
    "    print(\"training_loss_model_images_features: \",training_loss_images_features)\n",
    "\n",
    "elif data_to_learn_with == 'pca_features':\n",
    "    if load_model:\n",
    "        number_of_components_model_to_load = 167\n",
    "        rand_forrest.load_model_pca(number_of_components_model_to_load, n_estimators)\n",
    "    else:\n",
    "        if train_with_cross_validation:\n",
    "            cross_validation_by_minimizing_all_features_loss = True\n",
    "            if cross_validation_by_minimizing_all_features_loss:  # If True then the model will find the number of components that minimizes the error of all data and will be trained on all the data\n",
    "                rand_forrest.train_model_pca_cross_validation()        #The model finds the number of PCA components that minimizes the validation error\n",
    "            else:                                                 #If False then the model will find the number of components that minimizes only the error of the validation features (80)\n",
    "                rand_forrest.train_model_pca_cross_validation('data_splited')   #The model finds the number of PCA components that minimizes the validation data (20% of all data) and will be trained all 80% of the training data\n",
    "\n",
    "        else:\n",
    "            number_of_components_to_train_with = 167              #It's a number between 1-192\n",
    "            rand_forrest.train_model_pca(num_comp=number_of_components_to_train_with, n_estimators=n_estimators)\n",
    "    \n",
    "    training_loss_features_pca = rand_forrest.calculate_training_loss_pca_data()\n",
    "    print(\"training_loss_model_PCA: \",training_loss_features_pca)\n",
    "else:\n",
    "    print(\"ERROR, data_to_learn_with must be : 'features', 'images', 'features_images' or 'pca_features' \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forth Model : Gaussian Naive Bayes classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss_model_PCA:  0.01751195247170267\n"
     ]
    }
   ],
   "source": [
    "gnbayes = gnb.GaussianNaiveBayesModel()\n",
    "\n",
    "data_to_learn_with = 'pca_features'\n",
    "\n",
    "load_model = True    # If true then we will load a model that is already trained, else we will train a model\n",
    "\n",
    "if data_to_learn_with == 'features':\n",
    "    if load_model:\n",
    "        gnbayes.load_model_features()\n",
    "    else:\n",
    "        gnbayes.train_model_features()\n",
    "\n",
    "    training_features_loss = gnbayes.calculate_training_loss()\n",
    "    print(\"training_loss_model_features: \",training_features_loss)\n",
    "    \n",
    "elif data_to_learn_with == 'images':\n",
    "    if load_model:\n",
    "        gnbayes.load_model_images()\n",
    "    else:\n",
    "        gnbayes.train_model_images()\n",
    "\n",
    "    training_loss_images = gnbayes.calculate_training_loss_images()\n",
    "    print(\"training_loss_model_images: \",training_loss_images)\n",
    "    \n",
    "elif data_to_learn_with == 'features_images':\n",
    "    if load_model: \n",
    "        gnbayes.load_model_features_images()\n",
    "    else:\n",
    "        gnbayes.train_model_images_features()\n",
    "        \n",
    "    training_loss_images_features = gnbayes.calculate_training_loss_features_images()\n",
    "    print(\"training_loss_model_images_features: \",training_loss_images_features)\n",
    "\n",
    "elif data_to_learn_with == 'pca_features':\n",
    "    if load_model:\n",
    "        number_of_components_model_to_load = 19\n",
    "        gnbayes.load_model_pca(number_of_components_model_to_load)\n",
    "    else:\n",
    "        train_with_cross_validation = False\n",
    "        if train_with_cross_validation:\n",
    "            cross_validation_by_minimizing_all_features_loss = True\n",
    "            if cross_validation_by_minimizing_all_features_loss:  # If True then the model will find the number of components that minimizes the error of all data and will be trained on all the data\n",
    "                gnbayes.train_model_pca_cross_validation()        #The model finds the number of PCA components that minimizes the validation error\n",
    "            else:                                                 #If False then the model will find the number of components that minimizes only the error of the validation features (80)\n",
    "                gnbayes.train_model_pca_cross_validation('data_splited')   #The model finds the number of PCA components that minimizes the validation data (20% of all data) and will be trained all 80% of the training data\n",
    "\n",
    "        else:\n",
    "            number_of_components_to_train_with = 19              #It's a number between 1-192\n",
    "            gnbayes.train_model_pca(num_comp=number_of_components_to_train_with)\n",
    "    \n",
    "    training_loss_features_pca = gnbayes.calculate_training_loss_pca_data()\n",
    "    print(\"training_loss_model_PCA: \",training_loss_features_pca)\n",
    "\n",
    "else:\n",
    "    print(\"ERROR, data_to_learn_with must be : 'features', 'images', 'features_images' or 'pca_features' \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fifth Model : Support Vector Machine classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sixth Model : Neural Network classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
