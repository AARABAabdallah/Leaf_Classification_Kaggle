{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leaf Classification Kaggel Project\n",
    "\n",
    "## Implemented models :\n",
    "\n",
    "* Logistic Regression\n",
    "* Support Vector Machine\n",
    "* Neurenal network\n",
    "* AdaBoost\n",
    "* Random Forrest\n",
    "* Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    " Imporation des mod√®les\n",
    "'''\n",
    "import os,sys,inspect\n",
    "\n",
    "current_dir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "relative_path = parent_dir + \"/src\"\n",
    "sys.path.insert(0, relative_path)\n",
    "\n",
    "import models.logistical_regression_model as lr\n",
    "import models.adaBoost_model as ab\n",
    "import models.randomForrest_model as rf\n",
    "import models.gaussianNaiveBayes_model as gnb\n",
    "import models.svm_model as svc\n",
    "import models.RN_sklearn_model as RN_skl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Model : Logistic regression classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = lr.LogisticalRegressionModel()\n",
    "\n",
    "data_to_learn_with = 'features'\n",
    "\n",
    "load_model = True    # If true then we will load a model that is already trained, else we will train a model\n",
    "\n",
    "if data_to_learn_with == 'features':\n",
    "    if load_model:\n",
    "        log_reg.load_model_features()\n",
    "    else:\n",
    "        log_reg.train_model_features()\n",
    "\n",
    "    training_features_loss = log_reg.calculate_training_loss()\n",
    "    print(\"training_loss_model_features: \",training_features_loss)\n",
    "    \n",
    "elif data_to_learn_with == 'images':\n",
    "    if load_model:\n",
    "        log_reg.load_model_images()\n",
    "    else:\n",
    "        log_reg.train_model_images()\n",
    "\n",
    "    training_loss_images = log_reg.calculate_training_loss_images()\n",
    "    print(\"training_loss_model_images: \",training_loss_images)\n",
    "    \n",
    "elif data_to_learn_with == 'features_images':\n",
    "    if load_model: \n",
    "        log_reg.load_model_features_images()\n",
    "    else:\n",
    "        log_reg.train_model_images_features()\n",
    "        \n",
    "    training_loss_images_features = log_reg.calculate_training_loss_features_images()\n",
    "    print(\"training_loss_model_images_features: \",training_loss_images_features)\n",
    "\n",
    "elif data_to_learn_with == 'pca_features':\n",
    "    if load_model:\n",
    "        number_of_components_model_to_load = 167\n",
    "        log_reg.load_model_pca(number_of_components_model_to_load)\n",
    "    else:\n",
    "        train_with_cross_validation = False\n",
    "        if train_with_cross_validation:\n",
    "            cross_validation_by_minimizing_all_features_loss = True\n",
    "            if cross_validation_by_minimizing_all_features_loss:  # If True then the model will find the number of components that minimizes the error of all data and will be trained on all the data\n",
    "                log_reg.train_model_pca_cross_validation()        #The model finds the number of PCA components that minimizes the validation error\n",
    "            else:                                                 #If False then the model will find the number of components that minimizes only the error of the validation features (80)\n",
    "                log_reg.train_model_pca_cross_validation('data_splited')   #The model finds the number of PCA components that minimizes the validation data (20% of all data) and will be trained all 80% of the training data\n",
    "\n",
    "        else:\n",
    "            number_of_components_to_train_with = 167              #It's a number between 1-192\n",
    "            log_reg.train_model_pca(num_comp=number_of_components_to_train_with)\n",
    "    \n",
    "    training_loss_features_pca = log_reg.calculate_training_loss_pca_data()\n",
    "    print(\"training_loss_model_PCA: \",training_loss_features_pca)\n",
    "\n",
    "else:\n",
    "    print(\"ERROR, data_to_learn_with must be : 'features', 'images', 'features_images' or 'pca_features' \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second Model : AdaBoost classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada_boost = ab.AdaBoostModel()\n",
    "\n",
    "data_to_learn_with = 'pca_features'\n",
    "\n",
    "load_model = True    # If true then we will load a model that is already trained, else we will train a model\n",
    "train_with_cross_validation = False #Cross Validation upon the number of estimators\n",
    "n_estimators = 1000\n",
    "\n",
    "if data_to_learn_with == 'features':\n",
    "    if load_model:\n",
    "        ada_boost.load_model_features(n_estimators)   #Load the model that has been trained with number of estimators = n_estimators\n",
    "    else:\n",
    "        if train_with_cross_validation:\n",
    "            ada_boost.train_model_features_cross_validation()\n",
    "        else:\n",
    "            ada_boost.train_model_features(n_estimators)\n",
    "\n",
    "    training_features_loss = ada_boost.calculate_training_loss()\n",
    "    print(\"training_loss_model_features: \",training_features_loss)\n",
    "    \n",
    "elif data_to_learn_with == 'images':\n",
    "    if load_model:\n",
    "        ada_boost.load_model_images(n_estimators)   #Load the model that has been trained with number of estimators = n_estimators\n",
    "    else:\n",
    "        if train_with_cross_validation:\n",
    "            ada_boost.train_model_images_cross_validation()\n",
    "        else:\n",
    "            ada_boost.train_model_images(n_estimators)\n",
    "\n",
    "    training_loss_images = ada_boost.calculate_training_loss_images()\n",
    "    print(\"training_loss_model_images: \",training_loss_images)\n",
    "    \n",
    "elif data_to_learn_with == 'features_images':\n",
    "    if load_model:\n",
    "        ada_boost.load_model_features_images(n_estimators)   #Load the model that has been trained with number of estimators = n_estimators\n",
    "    else:\n",
    "        if train_with_cross_validation:\n",
    "            ada_boost.train_model_features_images_cross_validation()\n",
    "        else:\n",
    "            ada_boost.train_model_images_features(n_estimators)\n",
    "        \n",
    "    training_loss_images_features = ada_boost.calculate_training_loss_features_images()\n",
    "    print(\"training_loss_model_images_features: \",training_loss_images_features)\n",
    "\n",
    "elif data_to_learn_with == 'pca_features':\n",
    "    if load_model:\n",
    "        number_of_components_model_to_load = 167\n",
    "        ada_boost.load_model_pca(number_of_components_model_to_load, n_estimators)\n",
    "    else:\n",
    "        if train_with_cross_validation:\n",
    "            cross_validation_by_minimizing_all_features_loss = True\n",
    "            if cross_validation_by_minimizing_all_features_loss:  # If True then the model will find the number of components that minimizes the error of all data and will be trained on all the data\n",
    "                ada_boost.train_model_pca_cross_validation()        #The model finds the number of PCA components that minimizes the validation error\n",
    "            else:                                                 #If False then the model will find the number of components that minimizes only the error of the validation features (80)\n",
    "                ada_boost.train_model_pca_cross_validation('data_splited')   #The model finds the number of PCA components that minimizes the validation data (20% of all data) and will be trained all 80% of the training data\n",
    "\n",
    "        else:\n",
    "            number_of_components_to_train_with = 167              #It's a number between 1-192\n",
    "            ada_boost.train_model_pca(num_comp=number_of_components_to_train_with, n_estimators=n_estimators)\n",
    "    \n",
    "    training_loss_features_pca = ada_boost.calculate_training_loss_pca_data()\n",
    "    print(\"training_loss_model_PCA: \",training_loss_features_pca)\n",
    "else:\n",
    "    print(\"ERROR, data_to_learn_with must be : 'features', 'images', 'features_images' or 'pca_features' \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Third Model : Random Forrest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_forrest = rf.RandomForrestModel()\n",
    "\n",
    "data_to_learn_with = 'pca_features'\n",
    "\n",
    "load_model = False    # If true then we will load a model that is already trained, else we will train a model\n",
    "train_with_cross_validation = False #Cross Validation upon the number of estimators\n",
    "n_estimators = 300\n",
    "\n",
    "if data_to_learn_with == 'features':\n",
    "    if load_model:\n",
    "        rand_forrest.load_model_features(n_estimators)   #Load the model that has been trained with number of estimators = n_estimators\n",
    "    else:\n",
    "        if train_with_cross_validation:\n",
    "            rand_forrest.train_model_features_cross_validation()\n",
    "        else:\n",
    "            rand_forrest.train_model_features(n_estimators)\n",
    "\n",
    "    training_features_loss = rand_forrest.calculate_training_loss()\n",
    "    print(\"training_loss_model_features: \",training_features_loss)\n",
    "    \n",
    "elif data_to_learn_with == 'images':\n",
    "    if load_model:\n",
    "        rand_forrest.load_model_images(n_estimators)   #Load the model that has been trained with number of estimators = n_estimators\n",
    "    else:\n",
    "        if train_with_cross_validation:\n",
    "            rand_forrest.train_model_images_cross_validation()\n",
    "        else:\n",
    "            rand_forrest.train_model_images(n_estimators)\n",
    "\n",
    "    training_loss_images = rand_forrest.calculate_training_loss_images()\n",
    "    print(\"training_loss_model_images: \",training_loss_images)\n",
    "    \n",
    "elif data_to_learn_with == 'features_images':\n",
    "    if load_model:\n",
    "        rand_forrest.load_model_features_images(n_estimators)   #Load the model that has been trained with number of estimators = n_estimators\n",
    "    else:\n",
    "        if train_with_cross_validation:\n",
    "            rand_forrest.train_model_features_images_cross_validation()\n",
    "        else:\n",
    "            rand_forrest.train_model_images_features(n_estimators)\n",
    "        \n",
    "    training_loss_images_features = rand_forrest.calculate_training_loss_features_images()\n",
    "    print(\"training_loss_model_images_features: \",training_loss_images_features)\n",
    "\n",
    "elif data_to_learn_with == 'pca_features':\n",
    "    if load_model:\n",
    "        number_of_components_model_to_load = 167\n",
    "        rand_forrest.load_model_pca(number_of_components_model_to_load, n_estimators)\n",
    "    else:\n",
    "        if train_with_cross_validation:\n",
    "            cross_validation_by_minimizing_all_features_loss = True\n",
    "            if cross_validation_by_minimizing_all_features_loss:  # If True then the model will find the number of components that minimizes the error of all data and will be trained on all the data\n",
    "                rand_forrest.train_model_pca_cross_validation()        #The model finds the number of PCA components that minimizes the validation error\n",
    "            else:                                                 #If False then the model will find the number of components that minimizes only the error of the validation features (80)\n",
    "                rand_forrest.train_model_pca_cross_validation('data_splited')   #The model finds the number of PCA components that minimizes the validation data (20% of all data) and will be trained all 80% of the training data\n",
    "\n",
    "        else:\n",
    "            number_of_components_to_train_with = 167              #It's a number between 1-192\n",
    "            rand_forrest.train_model_pca(num_comp=number_of_components_to_train_with, n_estimators=n_estimators)\n",
    "    \n",
    "    training_loss_features_pca = rand_forrest.calculate_training_loss_pca_data()\n",
    "    print(\"training_loss_model_PCA: \",training_loss_features_pca)\n",
    "else:\n",
    "    print(\"ERROR, data_to_learn_with must be : 'features', 'images', 'features_images' or 'pca_features' \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forth Model : Gaussian Naive Bayes classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnbayes = gnb.GaussianNaiveBayesModel()\n",
    "\n",
    "data_to_learn_with = 'pca_features'\n",
    "\n",
    "load_model = True    # If true then we will load a model that is already trained, else we will train a model\n",
    "\n",
    "if data_to_learn_with == 'features':\n",
    "    if load_model:\n",
    "        gnbayes.load_model_features()\n",
    "    else:\n",
    "        gnbayes.train_model_features()\n",
    "\n",
    "    training_features_loss = gnbayes.calculate_training_loss()\n",
    "    print(\"training_loss_model_features: \",training_features_loss)\n",
    "    \n",
    "elif data_to_learn_with == 'images':\n",
    "    if load_model:\n",
    "        gnbayes.load_model_images()\n",
    "    else:\n",
    "        gnbayes.train_model_images()\n",
    "\n",
    "    training_loss_images = gnbayes.calculate_training_loss_images()\n",
    "    print(\"training_loss_model_images: \",training_loss_images)\n",
    "    \n",
    "elif data_to_learn_with == 'features_images':\n",
    "    if load_model: \n",
    "        gnbayes.load_model_features_images()\n",
    "    else:\n",
    "        gnbayes.train_model_images_features()\n",
    "        \n",
    "    training_loss_images_features = gnbayes.calculate_training_loss_features_images()\n",
    "    print(\"training_loss_model_images_features: \",training_loss_images_features)\n",
    "\n",
    "elif data_to_learn_with == 'pca_features':\n",
    "    if load_model:\n",
    "        number_of_components_model_to_load = 19\n",
    "        gnbayes.load_model_pca(number_of_components_model_to_load)\n",
    "    else:\n",
    "        train_with_cross_validation = False\n",
    "        if train_with_cross_validation:\n",
    "            cross_validation_by_minimizing_all_features_loss = True\n",
    "            if cross_validation_by_minimizing_all_features_loss:  # If True then the model will find the number of components that minimizes the error of all data and will be trained on all the data\n",
    "                gnbayes.train_model_pca_cross_validation()        #The model finds the number of PCA components that minimizes the validation error\n",
    "            else:                                                 #If False then the model will find the number of components that minimizes only the error of the validation features (80)\n",
    "                gnbayes.train_model_pca_cross_validation('data_splited')   #The model finds the number of PCA components that minimizes the validation data (20% of all data) and will be trained all 80% of the training data\n",
    "\n",
    "        else:\n",
    "            number_of_components_to_train_with = 19              #It's a number between 1-192\n",
    "            gnbayes.train_model_pca(num_comp=number_of_components_to_train_with)\n",
    "    \n",
    "    training_loss_features_pca = gnbayes.calculate_training_loss_pca_data()\n",
    "    print(\"training_loss_model_PCA: \",training_loss_features_pca)\n",
    "\n",
    "else:\n",
    "    print(\"ERROR, data_to_learn_with must be : 'features', 'images', 'features_images' or 'pca_features' \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fifth Model : Support Vector Machine classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_model = svc.SvmModel()\n",
    "# Disponible kernels : 'default' ( linear ), 'rbf' , 'poly', 'sigmoid'\n",
    "kernel_to_learn_with = 'sigmoid'\n",
    "data_to_learn_with = \"features\"\n",
    "load_model = False    # If true then we will load a model that is already trained, else we will train a model\n",
    "submit = True #to save the file that serves to submit the model or not\n",
    "\n",
    "if data_to_learn_with == 'features':\n",
    "    if load_model:\n",
    "        svm_model.load_model(kernel = kernel_to_learn_with)\n",
    "    else:\n",
    "        cross_validate = False\n",
    "        if cross_validate:\n",
    "            svm_model.cross_validation(kernel = kernel_to_learn_with)\n",
    "        else:\n",
    "            degree = 100\n",
    "            gamma = 1\n",
    "            coef0=23\n",
    "            svm_model.train_model(kernel=kernel_to_learn_with,degree = degree,gamma = gamma,coef0=coef0)\n",
    "\n",
    "    training_features_loss = svm_model.calculate_training_loss()\n",
    "    print(\"training_loss_model_features: \",training_features_loss)\n",
    "\n",
    "    if submit:\n",
    "        svm_model.submit_test_results_features(kernel = kernel_to_learn_with)\n",
    "\n",
    "\n",
    "elif data_to_learn_with == 'pca_features':\n",
    "    if load_model:\n",
    "        number_of_components_model_to_load = 160 #best value given by cross validatin method\n",
    "        svm_model.load_model_pca(number_of_components_model_to_load)\n",
    "    else:\n",
    "        train_with_cross_validation = False\n",
    "        if train_with_cross_validation:\n",
    "            cross_validation_by_minimizing_all_features_loss = True\n",
    "            if cross_validation_by_minimizing_all_features_loss:  # If True then the model will find the number of components that minimizes the error of all data and will be trained on all the data\n",
    "                svm_model.train_model_pca_cross_validation()        #The model finds the number of PCA components that minimizes the validation error\n",
    "            else:                                                 #If False then the model will find the number of components that minimizes only the error of the validation features (80)\n",
    "                svm_model.train_model_pca_cross_validation('data_splited')   #The model finds the number of PCA components that minimizes the validation data (20% of all data) and will be trained all 80% of the training data\n",
    "\n",
    "        else:\n",
    "            number_of_components_to_train_with = 160             #It's a number between 1-192\n",
    "            svm_model.train_model_pca(num_comp=number_of_components_to_train_with)\n",
    "    \n",
    "    training_loss_features_pca = svm_model.get_training_loss_pca_data()\n",
    "    print(\"training_loss_model_PCA: \",training_loss_features_pca)\n",
    "\n",
    "    if submit:\n",
    "        svm_model.submit_test_results_pca(kernel=kernel_to_learn_with)\n",
    "\n",
    "else:\n",
    "    print(\"ERROR, data_to_learn_with must be : 'features' or 'pca_features' \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sixth Model : Neural Network classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rn_skl = RN_skl.RN_sklearn_model()\n",
    "\n",
    "data_to_learn_with = 'images'\n",
    "\n",
    "load_model = True    # If true then we will load a model that is already trained, else we will train a model\n",
    "submit = True #to save the file that serves to submit the model or not\n",
    "\n",
    "if data_to_learn_with == 'features':\n",
    "    if load_model:\n",
    "        rn_skl.load_model_features()\n",
    "    else:\n",
    "        train_with_cross_validation = False\n",
    "        if train_with_cross_validation:\n",
    "            layer1_range = range(40,180,10)\n",
    "            layer2_range = range(40,180,20)\n",
    "            layer3_range = range(40,180,30)\n",
    "            rn_skl.features_cross_valid(layer1_range=layer1_range,layer2_range=layer2_range,layer3_range=layer3_range)\n",
    "        else:\n",
    "            rn_skl.train_model_features()\n",
    "\n",
    "    training_features_loss = rn_skl.get_training_loss()\n",
    "    print(\"training_loss_model_features: \",training_features_loss)\n",
    "\n",
    "    if submit:\n",
    "        rn_skl.submit_test_results_features()\n",
    "    \n",
    "elif data_to_learn_with == 'images':\n",
    "    if load_model:\n",
    "        rn_skl.load_model_images()\n",
    "    else:\n",
    "        rn_skl.train_model_images()\n",
    "\n",
    "    training_loss_images = rn_skl.get_training_loss_images()\n",
    "    print(\"training_loss_model_images: \",training_loss_images)\n",
    "\n",
    "    if submit:\n",
    "        rn_skl.submit_test_results_images()\n",
    "    \n",
    "elif data_to_learn_with == 'features_images':\n",
    "    if load_model: \n",
    "        rn_skl.load_model_features_images()\n",
    "    else:\n",
    "        rn_skl.train_model_images_features()\n",
    "        \n",
    "    training_loss_images_features = rn_skl.get_training_loss_features_images()\n",
    "    print(\"training_loss_model_images_features: \",training_loss_images_features)\n",
    "\n",
    "    if submit:\n",
    "        rn_skl.submit_test_results_images_features()\n",
    "\n",
    "elif data_to_learn_with == 'pca_features':\n",
    "    if load_model:\n",
    "        number_of_components_model_to_load = 24 #best value given by cross validatin method\n",
    "        rn_skl.load_model_pca(number_of_components_model_to_load)\n",
    "    else:\n",
    "        train_with_cross_validation = False\n",
    "        if train_with_cross_validation:\n",
    "            cross_validation_by_minimizing_all_features_loss = True\n",
    "            if cross_validation_by_minimizing_all_features_loss:  # If True then the model will find the number of components that minimizes the error of all data and will be trained on all the data\n",
    "                rn_skl.train_model_pca_cross_validation()        #The model finds the number of PCA components that minimizes the validation error\n",
    "            else:                                                 #If False then the model will find the number of components that minimizes only the error of the validation features (80)\n",
    "                rn_skl.train_model_pca_cross_validation('data_splited')   #The model finds the number of PCA components that minimizes the validation data (20% of all data) and will be trained all 80% of the training data\n",
    "\n",
    "        else:\n",
    "            number_of_components_to_train_with = 24              #It's a number between 1-192\n",
    "            rn_skl.train_model_pca(num_comp=number_of_components_to_train_with)\n",
    "    \n",
    "    training_loss_features_pca = rn_skl.get_training_loss_pca_data()\n",
    "    print(\"training_loss_model_PCA: \",training_loss_features_pca)\n",
    "\n",
    "    if submit:\n",
    "        rn_skl.submit_test_results_pca()\n",
    "\n",
    "else:\n",
    "    print(\"ERROR, data_to_learn_with must be : 'features', 'images', 'features_images' or 'pca_features' \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
