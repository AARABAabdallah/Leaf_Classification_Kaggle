{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leaf Classification Kaggel Project\n",
    "\n",
    "## Implemented models :\n",
    "\n",
    "* Logistic Regression\n",
    "* Support Vector Machine\n",
    "* Neurenal network\n",
    "* AdaBoost\n",
    "* Random Forrest\n",
    "* Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SAFAE\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    " Imporation des mod√®les\n",
    "'''\n",
    "import os,sys,inspect\n",
    "\n",
    "current_dir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "relative_path = parent_dir + \"/src\"\n",
    "sys.path.insert(0, relative_path)\n",
    "\n",
    "import models.logistical_regression_model as lr\n",
    "import models.adaBoost_model as ab\n",
    "import models.randomForrest_model as rf\n",
    "import models.gaussianNaiveBayes_model as gnb\n",
    "import models.svm_model as svc\n",
    "import models.RN_sklearn_model as RN_skl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Model : Logistic regression classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss_model_features:  1.2979024732996943e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SAFAE\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:311: UserWarning: Trying to unpickle estimator LogisticRegression from version 0.21.3 when using version 0.19.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "log_reg = lr.LogisticalRegressionModel()\n",
    "\n",
    "data_to_learn_with = 'features'\n",
    "\n",
    "load_model = True    # If true then we will load a model that is already trained, else we will train a model\n",
    "\n",
    "if data_to_learn_with == 'features':\n",
    "    if load_model:\n",
    "        log_reg.load_model_features()\n",
    "    else:\n",
    "        log_reg.train_model_features()\n",
    "\n",
    "    training_features_loss = log_reg.calculate_training_loss()\n",
    "    print(\"training_loss_model_features: \",training_features_loss)\n",
    "    \n",
    "elif data_to_learn_with == 'images':\n",
    "    if load_model:\n",
    "        log_reg.load_model_images()\n",
    "    else:\n",
    "        log_reg.train_model_images()\n",
    "\n",
    "    training_loss_images = log_reg.calculate_training_loss_images()\n",
    "    print(\"training_loss_model_images: \",training_loss_images)\n",
    "    \n",
    "elif data_to_learn_with == 'features_images':\n",
    "    if load_model: \n",
    "        log_reg.load_model_features_images()\n",
    "    else:\n",
    "        log_reg.train_model_images_features()\n",
    "        \n",
    "    training_loss_images_features = log_reg.calculate_training_loss_features_images()\n",
    "    print(\"training_loss_model_images_features: \",training_loss_images_features)\n",
    "\n",
    "elif data_to_learn_with == 'pca_features':\n",
    "    if load_model:\n",
    "        number_of_components_model_to_load = 167\n",
    "        log_reg.load_model_pca(number_of_components_model_to_load)\n",
    "    else:\n",
    "        train_with_cross_validation = False\n",
    "        if train_with_cross_validation:\n",
    "            cross_validation_by_minimizing_all_features_loss = True\n",
    "            if cross_validation_by_minimizing_all_features_loss:  # If True then the model will find the number of components that minimizes the error of all data and will be trained on all the data\n",
    "                log_reg.train_model_pca_cross_validation()        #The model finds the number of PCA components that minimizes the validation error\n",
    "            else:                                                 #If False then the model will find the number of components that minimizes only the error of the validation features (80)\n",
    "                log_reg.train_model_pca_cross_validation('data_splited')   #The model finds the number of PCA components that minimizes the validation data (20% of all data) and will be trained all 80% of the training data\n",
    "\n",
    "        else:\n",
    "            number_of_components_to_train_with = 167              #It's a number between 1-192\n",
    "            log_reg.train_model_pca(num_comp=number_of_components_to_train_with)\n",
    "    \n",
    "    training_loss_features_pca = log_reg.calculate_training_loss_pca_data()\n",
    "    print(\"training_loss_model_PCA: \",training_loss_features_pca)\n",
    "\n",
    "else:\n",
    "    print(\"ERROR, data_to_learn_with must be : 'features', 'images', 'features_images' or 'pca_features' \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second Model : AdaBoost classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SAFAE\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:311: UserWarning: Trying to unpickle estimator DecisionTreeClassifier from version 0.21.3 when using version 0.19.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "C:\\Users\\SAFAE\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:311: UserWarning: Trying to unpickle estimator AdaBoostClassifier from version 0.21.3 when using version 0.19.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss_model_PCA:  4.59503113734191\n"
     ]
    }
   ],
   "source": [
    "ada_boost = ab.AdaBoostModel()\n",
    "\n",
    "data_to_learn_with = 'pca_features'\n",
    "\n",
    "load_model = True    # If true then we will load a model that is already trained, else we will train a model\n",
    "train_with_cross_validation = False #Cross Validation upon the number of estimators\n",
    "n_estimators = 1000\n",
    "\n",
    "if data_to_learn_with == 'features':\n",
    "    if load_model:\n",
    "        ada_boost.load_model_features(n_estimators)   #Load the model that has been trained with number of estimators = n_estimators\n",
    "    else:\n",
    "        if train_with_cross_validation:\n",
    "            ada_boost.train_model_features_cross_validation()\n",
    "        else:\n",
    "            ada_boost.train_model_features(n_estimators)\n",
    "\n",
    "    training_features_loss = ada_boost.calculate_training_loss()\n",
    "    print(\"training_loss_model_features: \",training_features_loss)\n",
    "    \n",
    "elif data_to_learn_with == 'images':\n",
    "    if load_model:\n",
    "        ada_boost.load_model_images(n_estimators)   #Load the model that has been trained with number of estimators = n_estimators\n",
    "    else:\n",
    "        if train_with_cross_validation:\n",
    "            ada_boost.train_model_images_cross_validation()\n",
    "        else:\n",
    "            ada_boost.train_model_images(n_estimators)\n",
    "\n",
    "    training_loss_images = ada_boost.calculate_training_loss_images()\n",
    "    print(\"training_loss_model_images: \",training_loss_images)\n",
    "    \n",
    "elif data_to_learn_with == 'features_images':\n",
    "    if load_model:\n",
    "        ada_boost.load_model_features_images(n_estimators)   #Load the model that has been trained with number of estimators = n_estimators\n",
    "    else:\n",
    "        if train_with_cross_validation:\n",
    "            ada_boost.train_model_features_images_cross_validation()\n",
    "        else:\n",
    "            ada_boost.train_model_images_features(n_estimators)\n",
    "        \n",
    "    training_loss_images_features = ada_boost.calculate_training_loss_features_images()\n",
    "    print(\"training_loss_model_images_features: \",training_loss_images_features)\n",
    "\n",
    "elif data_to_learn_with == 'pca_features':\n",
    "    if load_model:\n",
    "        number_of_components_model_to_load = 167\n",
    "        ada_boost.load_model_pca(number_of_components_model_to_load, n_estimators)\n",
    "    else:\n",
    "        if train_with_cross_validation:\n",
    "            cross_validation_by_minimizing_all_features_loss = True\n",
    "            if cross_validation_by_minimizing_all_features_loss:  # If True then the model will find the number of components that minimizes the error of all data and will be trained on all the data\n",
    "                ada_boost.train_model_pca_cross_validation()        #The model finds the number of PCA components that minimizes the validation error\n",
    "            else:                                                 #If False then the model will find the number of components that minimizes only the error of the validation features (80)\n",
    "                ada_boost.train_model_pca_cross_validation('data_splited')   #The model finds the number of PCA components that minimizes the validation data (20% of all data) and will be trained all 80% of the training data\n",
    "\n",
    "        else:\n",
    "            number_of_components_to_train_with = 167              #It's a number between 1-192\n",
    "            ada_boost.train_model_pca(num_comp=number_of_components_to_train_with, n_estimators=n_estimators)\n",
    "    \n",
    "    training_loss_features_pca = ada_boost.calculate_training_loss_pca_data()\n",
    "    print(\"training_loss_model_PCA: \",training_loss_features_pca)\n",
    "else:\n",
    "    print(\"ERROR, data_to_learn_with must be : 'features', 'images', 'features_images' or 'pca_features' \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Third Model : Random Forrest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "EOFError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mEOFError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-37e9b7060d9b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     46\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mload_model\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m         \u001b[0mnumber_of_components_model_to_load\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m167\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 48\u001b[1;33m         \u001b[0mrand_forrest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_model_pca\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnumber_of_components_model_to_load\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_estimators\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     49\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtrain_with_cross_validation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Studys UDES\\IFT712 Techn. d apprentissage\\Leaf_Classification_Kaggle/src\\models\\randomForrest_model.py\u001b[0m in \u001b[0;36mload_model_pca\u001b[1;34m(self, num_comp, n_estimators)\u001b[0m\n\u001b[0;32m    262\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnbr_comp_pca_model_trained_with\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnum_comp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    263\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_estimators_pca_trained_with\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mn_estimators\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 264\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclf_pca_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'../models/randomForrest_pca_model_'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_comp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'_'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'.joblib'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    265\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    266\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0msubmit_test_results_pca\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\joblib\\numpy_pickle.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(filename, mmap_mode)\u001b[0m\n\u001b[0;32m    603\u001b[0m                     \u001b[1;32mreturn\u001b[0m \u001b[0mload_compatibility\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    604\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 605\u001b[1;33m                 \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_unpickle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmmap_mode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    606\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    607\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\joblib\\numpy_pickle.py\u001b[0m in \u001b[0;36m_unpickle\u001b[1;34m(fobj, filename, mmap_mode)\u001b[0m\n\u001b[0;32m    527\u001b[0m     \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    528\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 529\u001b[1;33m         \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0munpickler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    530\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0munpickler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompat_mode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m             warnings.warn(\"The file '%s' has been generated with a \"\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\pickle.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1046\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1047\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1048\u001b[1;33m                     \u001b[1;32mraise\u001b[0m \u001b[0mEOFError\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1049\u001b[0m                 \u001b[1;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbytes_types\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1050\u001b[0m                 \u001b[0mdispatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mEOFError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "rand_forrest = rf.RandomForrestModel()\n",
    "\n",
    "data_to_learn_with = 'pca_features'\n",
    "\n",
    "load_model = True    # If true then we will load a model that is already trained, else we will train a model\n",
    "train_with_cross_validation = False #Cross Validation upon the number of estimators\n",
    "n_estimators = 300\n",
    "\n",
    "if data_to_learn_with == 'features':\n",
    "    if load_model:\n",
    "        rand_forrest.load_model_features(n_estimators)   #Load the model that has been trained with number of estimators = n_estimators\n",
    "    else:\n",
    "        if train_with_cross_validation:\n",
    "            rand_forrest.train_model_features_cross_validation()\n",
    "        else:\n",
    "            rand_forrest.train_model_features(n_estimators)\n",
    "\n",
    "    training_features_loss = rand_forrest.calculate_training_loss()\n",
    "    print(\"training_loss_model_features: \",training_features_loss)\n",
    "    \n",
    "elif data_to_learn_with == 'images':\n",
    "    if load_model:\n",
    "        rand_forrest.load_model_images(n_estimators)   #Load the model that has been trained with number of estimators = n_estimators\n",
    "    else:\n",
    "        if train_with_cross_validation:\n",
    "            rand_forrest.train_model_images_cross_validation()\n",
    "        else:\n",
    "            rand_forrest.train_model_images(n_estimators)\n",
    "\n",
    "    training_loss_images = rand_forrest.calculate_training_loss_images()\n",
    "    print(\"training_loss_model_images: \",training_loss_images)\n",
    "    \n",
    "elif data_to_learn_with == 'features_images':\n",
    "    if load_model:\n",
    "        rand_forrest.load_model_features_images(n_estimators)   #Load the model that has been trained with number of estimators = n_estimators\n",
    "    else:\n",
    "        if train_with_cross_validation:\n",
    "            rand_forrest.train_model_features_images_cross_validation()\n",
    "        else:\n",
    "            rand_forrest.train_model_images_features(n_estimators)\n",
    "        \n",
    "    training_loss_images_features = rand_forrest.calculate_training_loss_features_images()\n",
    "    print(\"training_loss_model_images_features: \",training_loss_images_features)\n",
    "\n",
    "elif data_to_learn_with == 'pca_features':\n",
    "    if load_model:\n",
    "        number_of_components_model_to_load = 167\n",
    "        rand_forrest.load_model_pca(number_of_components_model_to_load, n_estimators)\n",
    "    else:\n",
    "        if train_with_cross_validation:\n",
    "            cross_validation_by_minimizing_all_features_loss = True\n",
    "            if cross_validation_by_minimizing_all_features_loss:  # If True then the model will find the number of components that minimizes the error of all data and will be trained on all the data\n",
    "                rand_forrest.train_model_pca_cross_validation()        #The model finds the number of PCA components that minimizes the validation error\n",
    "            else:                                                 #If False then the model will find the number of components that minimizes only the error of the validation features (80)\n",
    "                rand_forrest.train_model_pca_cross_validation('data_splited')   #The model finds the number of PCA components that minimizes the validation data (20% of all data) and will be trained all 80% of the training data\n",
    "\n",
    "        else:\n",
    "            number_of_components_to_train_with = 167              #It's a number between 1-192\n",
    "            rand_forrest.train_model_pca(num_comp=number_of_components_to_train_with, n_estimators=n_estimators)\n",
    "    \n",
    "    training_loss_features_pca = rand_forrest.calculate_training_loss_pca_data()\n",
    "    print(\"training_loss_model_PCA: \",training_loss_features_pca)\n",
    "else:\n",
    "    print(\"ERROR, data_to_learn_with must be : 'features', 'images', 'features_images' or 'pca_features' \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forth Model : Gaussian Naive Bayes classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnbayes = gnb.GaussianNaiveBayesModel()\n",
    "\n",
    "data_to_learn_with = 'pca_features'\n",
    "\n",
    "load_model = True    # If true then we will load a model that is already trained, else we will train a model\n",
    "\n",
    "if data_to_learn_with == 'features':\n",
    "    if load_model:\n",
    "        gnbayes.load_model_features()\n",
    "    else:\n",
    "        gnbayes.train_model_features()\n",
    "\n",
    "    training_features_loss = gnbayes.calculate_training_loss()\n",
    "    print(\"training_loss_model_features: \",training_features_loss)\n",
    "    \n",
    "elif data_to_learn_with == 'images':\n",
    "    if load_model:\n",
    "        gnbayes.load_model_images()\n",
    "    else:\n",
    "        gnbayes.train_model_images()\n",
    "\n",
    "    training_loss_images = gnbayes.calculate_training_loss_images()\n",
    "    print(\"training_loss_model_images: \",training_loss_images)\n",
    "    \n",
    "elif data_to_learn_with == 'features_images':\n",
    "    if load_model: \n",
    "        gnbayes.load_model_features_images()\n",
    "    else:\n",
    "        gnbayes.train_model_images_features()\n",
    "        \n",
    "    training_loss_images_features = gnbayes.calculate_training_loss_features_images()\n",
    "    print(\"training_loss_model_images_features: \",training_loss_images_features)\n",
    "\n",
    "elif data_to_learn_with == 'pca_features':\n",
    "    if load_model:\n",
    "        number_of_components_model_to_load = 19\n",
    "        gnbayes.load_model_pca(number_of_components_model_to_load)\n",
    "    else:\n",
    "        train_with_cross_validation = False\n",
    "        if train_with_cross_validation:\n",
    "            cross_validation_by_minimizing_all_features_loss = True\n",
    "            if cross_validation_by_minimizing_all_features_loss:  # If True then the model will find the number of components that minimizes the error of all data and will be trained on all the data\n",
    "                gnbayes.train_model_pca_cross_validation()        #The model finds the number of PCA components that minimizes the validation error\n",
    "            else:                                                 #If False then the model will find the number of components that minimizes only the error of the validation features (80)\n",
    "                gnbayes.train_model_pca_cross_validation('data_splited')   #The model finds the number of PCA components that minimizes the validation data (20% of all data) and will be trained all 80% of the training data\n",
    "\n",
    "        else:\n",
    "            number_of_components_to_train_with = 19              #It's a number between 1-192\n",
    "            gnbayes.train_model_pca(num_comp=number_of_components_to_train_with)\n",
    "    \n",
    "    training_loss_features_pca = gnbayes.calculate_training_loss_pca_data()\n",
    "    print(\"training_loss_model_PCA: \",training_loss_features_pca)\n",
    "\n",
    "else:\n",
    "    print(\"ERROR, data_to_learn_with must be : 'features', 'images', 'features_images' or 'pca_features' \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fifth Model : Support Vector Machine classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rn_skl' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-9b1e72dbaf7e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     52\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mload_model\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m         \u001b[0mnumber_of_components_model_to_load\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m24\u001b[0m \u001b[1;31m#best value given by cross validatin method\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m         \u001b[0mrn_skl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_model_pca\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnumber_of_components_model_to_load\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     55\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m         \u001b[0mtrain_with_cross_validation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'rn_skl' is not defined"
     ]
    }
   ],
   "source": [
    "svm_model = svc.SvmModel()\n",
    "\n",
    "kernel_to_learn_with = 'linear'\n",
    "dataSet_to_train_with = \"features\"\n",
    "load_model = True    # If true then we will load a model that is already trained, else we will train a model\n",
    "submit = True #to save the file that serves to submit the model or not\n",
    "\n",
    "if data_to_learn_with == 'features':\n",
    "    if load_model:\n",
    "        svm_model.load_model_features()\n",
    "    else:\n",
    "        train_with_cross_validation = False\n",
    "        if train_with_cross_validation:\n",
    "            layer1_range = range(40,180,10)\n",
    "            layer2_range = range(40,180,20)\n",
    "            layer3_range = range(40,180,30)\n",
    "            svm_model.features_cross_valid(layer1_range=layer1_range,layer2_range=layer2_range,layer3_range=layer3_range)\n",
    "        else:\n",
    "            svm_model.train_model_features()\n",
    "\n",
    "    training_features_loss = svm_model.get_training_loss()\n",
    "    print(\"training_loss_model_features: \",training_features_loss)\n",
    "\n",
    "    if submit:\n",
    "        svm_model.submit_test_results_features()\n",
    "    \n",
    "elif data_to_learn_with == 'images':\n",
    "    if load_model:\n",
    "        svm_model.load_model_images()\n",
    "    else:\n",
    "        svm_model.train_model_images()\n",
    "\n",
    "    training_loss_images = svm_model.get_training_loss_images()\n",
    "    print(\"training_loss_model_images: \",training_loss_images)\n",
    "\n",
    "    if submit:\n",
    "        svm_model.submit_test_results_images()\n",
    "    \n",
    "elif data_to_learn_with == 'features_images':\n",
    "    if load_model: \n",
    "        svm_model.load_model_features_images()\n",
    "    else:\n",
    "        svm_model.train_model_images_features()\n",
    "        \n",
    "    training_loss_images_features = svm_model.get_training_loss_features_images()\n",
    "    print(\"training_loss_model_images_features: \",training_loss_images_features)\n",
    "\n",
    "    if submit:\n",
    "        svm_model.submit_test_results_images_features()\n",
    "\n",
    "elif data_to_learn_with == 'pca_features':\n",
    "    if load_model:\n",
    "        number_of_components_model_to_load = 24 #best value given by cross validatin method\n",
    "        svm_model.load_model_pca(number_of_components_model_to_load)\n",
    "    else:\n",
    "        train_with_cross_validation = False\n",
    "        if train_with_cross_validation:\n",
    "            cross_validation_by_minimizing_all_features_loss = True\n",
    "            if cross_validation_by_minimizing_all_features_loss:  # If True then the model will find the number of components that minimizes the error of all data and will be trained on all the data\n",
    "                svm_model.train_model_pca_cross_validation()        #The model finds the number of PCA components that minimizes the validation error\n",
    "            else:                                                 #If False then the model will find the number of components that minimizes only the error of the validation features (80)\n",
    "                svm_model.train_model_pca_cross_validation('data_splited')   #The model finds the number of PCA components that minimizes the validation data (20% of all data) and will be trained all 80% of the training data\n",
    "\n",
    "        else:\n",
    "            number_of_components_to_train_with = 24              #It's a number between 1-192\n",
    "            svm_model.train_model_pca(num_comp=number_of_components_to_train_with)\n",
    "    \n",
    "    training_loss_features_pca = svm_model.get_training_loss_pca_data()\n",
    "    print(\"training_loss_model_PCA: \",training_loss_features_pca)\n",
    "\n",
    "    if submit:\n",
    "        svm_model.submit_test_results_pca()\n",
    "\n",
    "else:\n",
    "    print(\"ERROR, data_to_learn_with must be : 'features', 'images', 'features_images' or 'pca_features' \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sixth Model : Neural Network classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss_model_features:  0.17741072839880745\n"
     ]
    }
   ],
   "source": [
    "rn_skl = RN_skl.RN_sklearn_model()\n",
    "\n",
    "data_to_learn_with = 'features'\n",
    "\n",
    "load_model = True    # If true then we will load a model that is already trained, else we will train a model\n",
    "submit = True #to save the file that serves to submit the model or not\n",
    "\n",
    "if data_to_learn_with == 'features':\n",
    "    if load_model:\n",
    "        rn_skl.load_model_features()\n",
    "    else:\n",
    "        train_with_cross_validation = False\n",
    "        if train_with_cross_validation:\n",
    "            layer1_range = range(40,180,10)\n",
    "            layer2_range = range(40,180,20)\n",
    "            layer3_range = range(40,180,30)\n",
    "            rn_skl.features_cross_valid(layer1_range=layer1_range,layer2_range=layer2_range,layer3_range=layer3_range)\n",
    "        else:\n",
    "            rn_skl.train_model_features()\n",
    "\n",
    "    training_features_loss = rn_skl.get_training_loss()\n",
    "    print(\"training_loss_model_features: \",training_features_loss)\n",
    "\n",
    "    if submit:\n",
    "        rn_skl.submit_test_results_features()\n",
    "    \n",
    "elif data_to_learn_with == 'images':\n",
    "    if load_model:\n",
    "        rn_skl.load_model_images()\n",
    "    else:\n",
    "        rn_skl.train_model_images()\n",
    "\n",
    "    training_loss_images = rn_skl.get_training_loss_images()\n",
    "    print(\"training_loss_model_images: \",training_loss_images)\n",
    "\n",
    "    if submit:\n",
    "        rn_skl.submit_test_results_images()\n",
    "    \n",
    "elif data_to_learn_with == 'features_images':\n",
    "    if load_model: \n",
    "        rn_skl.load_model_features_images()\n",
    "    else:\n",
    "        rn_skl.train_model_images_features()\n",
    "        \n",
    "    training_loss_images_features = rn_skl.get_training_loss_features_images()\n",
    "    print(\"training_loss_model_images_features: \",training_loss_images_features)\n",
    "\n",
    "    if submit:\n",
    "        rn_skl.submit_test_results_images_features()\n",
    "\n",
    "elif data_to_learn_with == 'pca_features':\n",
    "    if load_model:\n",
    "        number_of_components_model_to_load = 24 #best value given by cross validatin method\n",
    "        rn_skl.load_model_pca(number_of_components_model_to_load)\n",
    "    else:\n",
    "        train_with_cross_validation = False\n",
    "        if train_with_cross_validation:\n",
    "            cross_validation_by_minimizing_all_features_loss = True\n",
    "            if cross_validation_by_minimizing_all_features_loss:  # If True then the model will find the number of components that minimizes the error of all data and will be trained on all the data\n",
    "                rn_skl.train_model_pca_cross_validation()        #The model finds the number of PCA components that minimizes the validation error\n",
    "            else:                                                 #If False then the model will find the number of components that minimizes only the error of the validation features (80)\n",
    "                rn_skl.train_model_pca_cross_validation('data_splited')   #The model finds the number of PCA components that minimizes the validation data (20% of all data) and will be trained all 80% of the training data\n",
    "\n",
    "        else:\n",
    "            number_of_components_to_train_with = 24              #It's a number between 1-192\n",
    "            rn_skl.train_model_pca(num_comp=number_of_components_to_train_with)\n",
    "    \n",
    "    training_loss_features_pca = rn_skl.get_training_loss_pca_data()\n",
    "    print(\"training_loss_model_PCA: \",training_loss_features_pca)\n",
    "\n",
    "    if submit:\n",
    "        rn_skl.submit_test_results_pca()\n",
    "\n",
    "else:\n",
    "    print(\"ERROR, data_to_learn_with must be : 'features', 'images', 'features_images' or 'pca_features' \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}