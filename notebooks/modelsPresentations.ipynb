{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leaf Classification Kaggel Project\n",
    "\n",
    "## Implemented models :\n",
    "\n",
    "* Logistic Regression\n",
    "* Support Vector Machine\n",
    "* Neurenal network\n",
    "* AdaBoost\n",
    "* Random Forrest\n",
    "* Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SAFAE\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    " Imporation des mod√®les\n",
    "'''\n",
    "import os,sys,inspect\n",
    "\n",
    "current_dir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "relative_path = parent_dir + \"/src\"\n",
    "sys.path.insert(0, relative_path)\n",
    "\n",
    "import models.logistical_regression_model as lr\n",
    "import models.adaBoost_model as ab\n",
    "import models.randomForrest_model as rf\n",
    "import models.gaussianNaiveBayes_model as gnb\n",
    "import models.svm_model as svc\n",
    "import models.RN_sklearn_model as RN_skl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Model : Logistic regression classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss_model_features:  1.2979024732996943e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SAFAE\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:311: UserWarning: Trying to unpickle estimator LogisticRegression from version 0.21.3 when using version 0.19.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "log_reg = lr.LogisticalRegressionModel()\n",
    "\n",
    "data_to_learn_with = 'features'\n",
    "\n",
    "load_model = True    # If true then we will load a model that is already trained, else we will train a model\n",
    "\n",
    "if data_to_learn_with == 'features':\n",
    "    if load_model:\n",
    "        log_reg.load_model_features()\n",
    "    else:\n",
    "        log_reg.train_model_features()\n",
    "\n",
    "    training_features_loss = log_reg.calculate_training_loss()\n",
    "    print(\"training_loss_model_features: \",training_features_loss)\n",
    "    \n",
    "elif data_to_learn_with == 'images':\n",
    "    if load_model:\n",
    "        log_reg.load_model_images()\n",
    "    else:\n",
    "        log_reg.train_model_images()\n",
    "\n",
    "    training_loss_images = log_reg.calculate_training_loss_images()\n",
    "    print(\"training_loss_model_images: \",training_loss_images)\n",
    "    \n",
    "elif data_to_learn_with == 'features_images':\n",
    "    if load_model: \n",
    "        log_reg.load_model_features_images()\n",
    "    else:\n",
    "        log_reg.train_model_images_features()\n",
    "        \n",
    "    training_loss_images_features = log_reg.calculate_training_loss_features_images()\n",
    "    print(\"training_loss_model_images_features: \",training_loss_images_features)\n",
    "\n",
    "elif data_to_learn_with == 'pca_features':\n",
    "    if load_model:\n",
    "        number_of_components_model_to_load = 167\n",
    "        log_reg.load_model_pca(number_of_components_model_to_load)\n",
    "    else:\n",
    "        train_with_cross_validation = False\n",
    "        if train_with_cross_validation:\n",
    "            cross_validation_by_minimizing_all_features_loss = True\n",
    "            if cross_validation_by_minimizing_all_features_loss:  # If True then the model will find the number of components that minimizes the error of all data and will be trained on all the data\n",
    "                log_reg.train_model_pca_cross_validation()        #The model finds the number of PCA components that minimizes the validation error\n",
    "            else:                                                 #If False then the model will find the number of components that minimizes only the error of the validation features (80)\n",
    "                log_reg.train_model_pca_cross_validation('data_splited')   #The model finds the number of PCA components that minimizes the validation data (20% of all data) and will be trained all 80% of the training data\n",
    "\n",
    "        else:\n",
    "            number_of_components_to_train_with = 167              #It's a number between 1-192\n",
    "            log_reg.train_model_pca(num_comp=number_of_components_to_train_with)\n",
    "    \n",
    "    training_loss_features_pca = log_reg.calculate_training_loss_pca_data()\n",
    "    print(\"training_loss_model_PCA: \",training_loss_features_pca)\n",
    "\n",
    "else:\n",
    "    print(\"ERROR, data_to_learn_with must be : 'features', 'images', 'features_images' or 'pca_features' \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second Model : AdaBoost classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SAFAE\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:311: UserWarning: Trying to unpickle estimator DecisionTreeClassifier from version 0.21.3 when using version 0.19.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "C:\\Users\\SAFAE\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:311: UserWarning: Trying to unpickle estimator AdaBoostClassifier from version 0.21.3 when using version 0.19.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss_model_PCA:  4.59503113734191\n"
     ]
    }
   ],
   "source": [
    "ada_boost = ab.AdaBoostModel()\n",
    "\n",
    "data_to_learn_with = 'pca_features'\n",
    "\n",
    "load_model = True    # If true then we will load a model that is already trained, else we will train a model\n",
    "train_with_cross_validation = False #Cross Validation upon the number of estimators\n",
    "n_estimators = 1000\n",
    "\n",
    "if data_to_learn_with == 'features':\n",
    "    if load_model:\n",
    "        ada_boost.load_model_features(n_estimators)   #Load the model that has been trained with number of estimators = n_estimators\n",
    "    else:\n",
    "        if train_with_cross_validation:\n",
    "            ada_boost.train_model_features_cross_validation()\n",
    "        else:\n",
    "            ada_boost.train_model_features(n_estimators)\n",
    "\n",
    "    training_features_loss = ada_boost.calculate_training_loss()\n",
    "    print(\"training_loss_model_features: \",training_features_loss)\n",
    "    \n",
    "elif data_to_learn_with == 'images':\n",
    "    if load_model:\n",
    "        ada_boost.load_model_images(n_estimators)   #Load the model that has been trained with number of estimators = n_estimators\n",
    "    else:\n",
    "        if train_with_cross_validation:\n",
    "            ada_boost.train_model_images_cross_validation()\n",
    "        else:\n",
    "            ada_boost.train_model_images(n_estimators)\n",
    "\n",
    "    training_loss_images = ada_boost.calculate_training_loss_images()\n",
    "    print(\"training_loss_model_images: \",training_loss_images)\n",
    "    \n",
    "elif data_to_learn_with == 'features_images':\n",
    "    if load_model:\n",
    "        ada_boost.load_model_features_images(n_estimators)   #Load the model that has been trained with number of estimators = n_estimators\n",
    "    else:\n",
    "        if train_with_cross_validation:\n",
    "            ada_boost.train_model_features_images_cross_validation()\n",
    "        else:\n",
    "            ada_boost.train_model_images_features(n_estimators)\n",
    "        \n",
    "    training_loss_images_features = ada_boost.calculate_training_loss_features_images()\n",
    "    print(\"training_loss_model_images_features: \",training_loss_images_features)\n",
    "\n",
    "elif data_to_learn_with == 'pca_features':\n",
    "    if load_model:\n",
    "        number_of_components_model_to_load = 167\n",
    "        ada_boost.load_model_pca(number_of_components_model_to_load, n_estimators)\n",
    "    else:\n",
    "        if train_with_cross_validation:\n",
    "            cross_validation_by_minimizing_all_features_loss = True\n",
    "            if cross_validation_by_minimizing_all_features_loss:  # If True then the model will find the number of components that minimizes the error of all data and will be trained on all the data\n",
    "                ada_boost.train_model_pca_cross_validation()        #The model finds the number of PCA components that minimizes the validation error\n",
    "            else:                                                 #If False then the model will find the number of components that minimizes only the error of the validation features (80)\n",
    "                ada_boost.train_model_pca_cross_validation('data_splited')   #The model finds the number of PCA components that minimizes the validation data (20% of all data) and will be trained all 80% of the training data\n",
    "\n",
    "        else:\n",
    "            number_of_components_to_train_with = 167              #It's a number between 1-192\n",
    "            ada_boost.train_model_pca(num_comp=number_of_components_to_train_with, n_estimators=n_estimators)\n",
    "    \n",
    "    training_loss_features_pca = ada_boost.calculate_training_loss_pca_data()\n",
    "    print(\"training_loss_model_PCA: \",training_loss_features_pca)\n",
    "else:\n",
    "    print(\"ERROR, data_to_learn_with must be : 'features', 'images', 'features_images' or 'pca_features' \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Third Model : Random Forrest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SAFAE\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:311: UserWarning: Trying to unpickle estimator DecisionTreeClassifier from version 0.21.3 when using version 0.19.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "C:\\Users\\SAFAE\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:311: UserWarning: Trying to unpickle estimator RandomForestClassifier from version 0.21.3 when using version 0.19.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'<' not supported between instances of 'NoneType' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-37e9b7060d9b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     59\u001b[0m             \u001b[0mrand_forrest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_model_pca\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_comp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnumber_of_components_to_train_with\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_estimators\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m     \u001b[0mtraining_loss_features_pca\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrand_forrest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcalculate_training_loss_pca_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"training_loss_model_PCA: \"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtraining_loss_features_pca\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Studys UDES\\IFT712 Techn. d apprentissage\\Leaf_Classification_Kaggle/src\\models\\randomForrest_model.py\u001b[0m in \u001b[0;36mcalculate_training_loss_pca_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    242\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_man\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_labels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    243\u001b[0m         \u001b[0mdata_pca_transformed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_man\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_data_pca_transformed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 244\u001b[1;33m         \u001b[0mprobas_pca_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclf_pca_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_pca_transformed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    245\u001b[0m         training_loss_pca_data = log_loss(y_true=labels, y_pred=probas_pca_data,\n\u001b[0;32m    246\u001b[0m                                                labels=self.clf_pca_model.classes_)\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    579\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    580\u001b[0m         \u001b[1;31m# Assign chunk of trees to jobs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 581\u001b[1;33m         \u001b[0mn_jobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_partition_estimators\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    582\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    583\u001b[0m         \u001b[1;31m# avoid storing the output of every estimator by summing them here\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\base.py\u001b[0m in \u001b[0;36m_partition_estimators\u001b[1;34m(n_estimators, n_jobs)\u001b[0m\n\u001b[0;32m    151\u001b[0m     \u001b[1;34m\"\"\"Private function used to partition estimators between jobs.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m     \u001b[1;31m# Compute the number of jobs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 153\u001b[1;33m     \u001b[0mn_jobs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_get_n_jobs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_estimators\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    154\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m     \u001b[1;31m# Partition estimators between jobs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\__init__.py\u001b[0m in \u001b[0;36m_get_n_jobs\u001b[1;34m(n_jobs)\u001b[0m\n\u001b[0;32m    462\u001b[0m     \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mParameter\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m \u001b[0mhas\u001b[0m \u001b[0mno\u001b[0m \u001b[0mmeaning\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    463\u001b[0m     \"\"\"\n\u001b[1;32m--> 464\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    465\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcpu_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    466\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: '<' not supported between instances of 'NoneType' and 'int'"
     ]
    }
   ],
   "source": [
    "rand_forrest = rf.RandomForrestModel()\n",
    "\n",
    "data_to_learn_with = 'pca_features'\n",
    "\n",
    "load_model = True    # If true then we will load a model that is already trained, else we will train a model\n",
    "train_with_cross_validation = False #Cross Validation upon the number of estimators\n",
    "n_estimators = 300\n",
    "\n",
    "if data_to_learn_with == 'features':\n",
    "    if load_model:\n",
    "        rand_forrest.load_model_features(n_estimators)   #Load the model that has been trained with number of estimators = n_estimators\n",
    "    else:\n",
    "        if train_with_cross_validation:\n",
    "            rand_forrest.train_model_features_cross_validation()\n",
    "        else:\n",
    "            rand_forrest.train_model_features(n_estimators)\n",
    "\n",
    "    training_features_loss = rand_forrest.calculate_training_loss()\n",
    "    print(\"training_loss_model_features: \",training_features_loss)\n",
    "    \n",
    "elif data_to_learn_with == 'images':\n",
    "    if load_model:\n",
    "        rand_forrest.load_model_images(n_estimators)   #Load the model that has been trained with number of estimators = n_estimators\n",
    "    else:\n",
    "        if train_with_cross_validation:\n",
    "            rand_forrest.train_model_images_cross_validation()\n",
    "        else:\n",
    "            rand_forrest.train_model_images(n_estimators)\n",
    "\n",
    "    training_loss_images = rand_forrest.calculate_training_loss_images()\n",
    "    print(\"training_loss_model_images: \",training_loss_images)\n",
    "    \n",
    "elif data_to_learn_with == 'features_images':\n",
    "    if load_model:\n",
    "        rand_forrest.load_model_features_images(n_estimators)   #Load the model that has been trained with number of estimators = n_estimators\n",
    "    else:\n",
    "        if train_with_cross_validation:\n",
    "            rand_forrest.train_model_features_images_cross_validation()\n",
    "        else:\n",
    "            rand_forrest.train_model_images_features(n_estimators)\n",
    "        \n",
    "    training_loss_images_features = rand_forrest.calculate_training_loss_features_images()\n",
    "    print(\"training_loss_model_images_features: \",training_loss_images_features)\n",
    "\n",
    "elif data_to_learn_with == 'pca_features':\n",
    "    if load_model:\n",
    "        number_of_components_model_to_load = 167\n",
    "        rand_forrest.load_model_pca(number_of_components_model_to_load, n_estimators)\n",
    "    else:\n",
    "        if train_with_cross_validation:\n",
    "            cross_validation_by_minimizing_all_features_loss = True\n",
    "            if cross_validation_by_minimizing_all_features_loss:  # If True then the model will find the number of components that minimizes the error of all data and will be trained on all the data\n",
    "                rand_forrest.train_model_pca_cross_validation()        #The model finds the number of PCA components that minimizes the validation error\n",
    "            else:                                                 #If False then the model will find the number of components that minimizes only the error of the validation features (80)\n",
    "                rand_forrest.train_model_pca_cross_validation('data_splited')   #The model finds the number of PCA components that minimizes the validation data (20% of all data) and will be trained all 80% of the training data\n",
    "\n",
    "        else:\n",
    "            number_of_components_to_train_with = 167              #It's a number between 1-192\n",
    "            rand_forrest.train_model_pca(num_comp=number_of_components_to_train_with, n_estimators=n_estimators)\n",
    "    \n",
    "    training_loss_features_pca = rand_forrest.calculate_training_loss_pca_data()\n",
    "    print(\"training_loss_model_PCA: \",training_loss_features_pca)\n",
    "else:\n",
    "    print(\"ERROR, data_to_learn_with must be : 'features', 'images', 'features_images' or 'pca_features' \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forth Model : Gaussian Naive Bayes classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss_model_PCA:  0.01751195247170264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SAFAE\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:311: UserWarning: Trying to unpickle estimator GaussianNB from version 0.21.3 when using version 0.19.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "gnbayes = gnb.GaussianNaiveBayesModel()\n",
    "\n",
    "data_to_learn_with = 'pca_features'\n",
    "\n",
    "load_model = True    # If true then we will load a model that is already trained, else we will train a model\n",
    "\n",
    "if data_to_learn_with == 'features':\n",
    "    if load_model:\n",
    "        gnbayes.load_model_features()\n",
    "    else:\n",
    "        gnbayes.train_model_features()\n",
    "\n",
    "    training_features_loss = gnbayes.calculate_training_loss()\n",
    "    print(\"training_loss_model_features: \",training_features_loss)\n",
    "    \n",
    "elif data_to_learn_with == 'images':\n",
    "    if load_model:\n",
    "        gnbayes.load_model_images()\n",
    "    else:\n",
    "        gnbayes.train_model_images()\n",
    "\n",
    "    training_loss_images = gnbayes.calculate_training_loss_images()\n",
    "    print(\"training_loss_model_images: \",training_loss_images)\n",
    "    \n",
    "elif data_to_learn_with == 'features_images':\n",
    "    if load_model: \n",
    "        gnbayes.load_model_features_images()\n",
    "    else:\n",
    "        gnbayes.train_model_images_features()\n",
    "        \n",
    "    training_loss_images_features = gnbayes.calculate_training_loss_features_images()\n",
    "    print(\"training_loss_model_images_features: \",training_loss_images_features)\n",
    "\n",
    "elif data_to_learn_with == 'pca_features':\n",
    "    if load_model:\n",
    "        number_of_components_model_to_load = 19\n",
    "        gnbayes.load_model_pca(number_of_components_model_to_load)\n",
    "    else:\n",
    "        train_with_cross_validation = False\n",
    "        if train_with_cross_validation:\n",
    "            cross_validation_by_minimizing_all_features_loss = True\n",
    "            if cross_validation_by_minimizing_all_features_loss:  # If True then the model will find the number of components that minimizes the error of all data and will be trained on all the data\n",
    "                gnbayes.train_model_pca_cross_validation()        #The model finds the number of PCA components that minimizes the validation error\n",
    "            else:                                                 #If False then the model will find the number of components that minimizes only the error of the validation features (80)\n",
    "                gnbayes.train_model_pca_cross_validation('data_splited')   #The model finds the number of PCA components that minimizes the validation data (20% of all data) and will be trained all 80% of the training data\n",
    "\n",
    "        else:\n",
    "            number_of_components_to_train_with = 19              #It's a number between 1-192\n",
    "            gnbayes.train_model_pca(num_comp=number_of_components_to_train_with)\n",
    "    \n",
    "    training_loss_features_pca = gnbayes.calculate_training_loss_pca_data()\n",
    "    print(\"training_loss_model_PCA: \",training_loss_features_pca)\n",
    "\n",
    "else:\n",
    "    print(\"ERROR, data_to_learn_with must be : 'features', 'images', 'features_images' or 'pca_features' \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fifth Model : Support Vector Machine classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss_model_features:  3.899278418810323\n"
     ]
    }
   ],
   "source": [
    "svm_model = svc.SvmModel()\n",
    "# Disponible kernels : 'default' ( linear ), 'rbf' , 'poly', 'sigmoid'\n",
    "kernel_to_learn_with = 'sigmoid'\n",
    "data_to_learn_with = \"features\"\n",
    "load_model = False    # If true then we will load a model that is already trained, else we will train a model\n",
    "submit = True #to save the file that serves to submit the model or not\n",
    "\n",
    "if data_to_learn_with == 'features':\n",
    "    if load_model:\n",
    "        svm_model.load_model(kernel = kernel_to_learn_with)\n",
    "    else:\n",
    "        cross_validate = False\n",
    "        if cross_validate:\n",
    "            svm_model.cross_validation(kernel = kernel_to_learn_with)\n",
    "        else:\n",
    "            degree = 100\n",
    "            gamma = 1\n",
    "            coef0=23\n",
    "            svm_model.train_model(kernel=kernel_to_learn_with,degree = degree,gamma = gamma,coef0=coef0)\n",
    "\n",
    "    training_features_loss = svm_model.calculate_training_loss()\n",
    "    print(\"training_loss_model_features: \",training_features_loss)\n",
    "\n",
    "    if submit:\n",
    "        svm_model.submit_test_results_features(kernel = kernel_to_learn_with)\n",
    "\n",
    "\n",
    "elif data_to_learn_with == 'pca_features':\n",
    "    if load_model:\n",
    "        number_of_components_model_to_load = 160 #best value given by cross validatin method\n",
    "        svm_model.load_model_pca(number_of_components_model_to_load)\n",
    "    else:\n",
    "        train_with_cross_validation = False\n",
    "        if train_with_cross_validation:\n",
    "            cross_validation_by_minimizing_all_features_loss = True\n",
    "            if cross_validation_by_minimizing_all_features_loss:  # If True then the model will find the number of components that minimizes the error of all data and will be trained on all the data\n",
    "                svm_model.train_model_pca_cross_validation()        #The model finds the number of PCA components that minimizes the validation error\n",
    "            else:                                                 #If False then the model will find the number of components that minimizes only the error of the validation features (80)\n",
    "                svm_model.train_model_pca_cross_validation('data_splited')   #The model finds the number of PCA components that minimizes the validation data (20% of all data) and will be trained all 80% of the training data\n",
    "\n",
    "        else:\n",
    "            number_of_components_to_train_with = 160             #It's a number between 1-192\n",
    "            svm_model.train_model_pca(num_comp=number_of_components_to_train_with)\n",
    "    \n",
    "    training_loss_features_pca = svm_model.get_training_loss_pca_data()\n",
    "    print(\"training_loss_model_PCA: \",training_loss_features_pca)\n",
    "\n",
    "    if submit:\n",
    "        svm_model.submit_test_results_pca(kernel=kernel_to_learn_with)\n",
    "\n",
    "else:\n",
    "    print(\"ERROR, data_to_learn_with must be : 'features' or 'pca_features' \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sixth Model : Neural Network classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss_model_images:  0.007827149677151533\n"
     ]
    }
   ],
   "source": [
    "rn_skl = RN_skl.RN_sklearn_model()\n",
    "\n",
    "data_to_learn_with = 'images'\n",
    "\n",
    "load_model = True    # If true then we will load a model that is already trained, else we will train a model\n",
    "submit = True #to save the file that serves to submit the model or not\n",
    "\n",
    "if data_to_learn_with == 'features':\n",
    "    if load_model:\n",
    "        rn_skl.load_model_features()\n",
    "    else:\n",
    "        train_with_cross_validation = False\n",
    "        if train_with_cross_validation:\n",
    "            layer1_range = range(40,180,10)\n",
    "            layer2_range = range(40,180,20)\n",
    "            layer3_range = range(40,180,30)\n",
    "            rn_skl.features_cross_valid(layer1_range=layer1_range,layer2_range=layer2_range,layer3_range=layer3_range)\n",
    "        else:\n",
    "            rn_skl.train_model_features()\n",
    "\n",
    "    training_features_loss = rn_skl.get_training_loss()\n",
    "    print(\"training_loss_model_features: \",training_features_loss)\n",
    "\n",
    "    if submit:\n",
    "        rn_skl.submit_test_results_features()\n",
    "    \n",
    "elif data_to_learn_with == 'images':\n",
    "    if load_model:\n",
    "        rn_skl.load_model_images()\n",
    "    else:\n",
    "        rn_skl.train_model_images()\n",
    "\n",
    "    training_loss_images = rn_skl.get_training_loss_images()\n",
    "    print(\"training_loss_model_images: \",training_loss_images)\n",
    "\n",
    "    if submit:\n",
    "        rn_skl.submit_test_results_images()\n",
    "    \n",
    "elif data_to_learn_with == 'features_images':\n",
    "    if load_model: \n",
    "        rn_skl.load_model_features_images()\n",
    "    else:\n",
    "        rn_skl.train_model_images_features()\n",
    "        \n",
    "    training_loss_images_features = rn_skl.get_training_loss_features_images()\n",
    "    print(\"training_loss_model_images_features: \",training_loss_images_features)\n",
    "\n",
    "    if submit:\n",
    "        rn_skl.submit_test_results_images_features()\n",
    "\n",
    "elif data_to_learn_with == 'pca_features':\n",
    "    if load_model:\n",
    "        number_of_components_model_to_load = 24 #best value given by cross validatin method\n",
    "        rn_skl.load_model_pca(number_of_components_model_to_load)\n",
    "    else:\n",
    "        train_with_cross_validation = False\n",
    "        if train_with_cross_validation:\n",
    "            cross_validation_by_minimizing_all_features_loss = True\n",
    "            if cross_validation_by_minimizing_all_features_loss:  # If True then the model will find the number of components that minimizes the error of all data and will be trained on all the data\n",
    "                rn_skl.train_model_pca_cross_validation()        #The model finds the number of PCA components that minimizes the validation error\n",
    "            else:                                                 #If False then the model will find the number of components that minimizes only the error of the validation features (80)\n",
    "                rn_skl.train_model_pca_cross_validation('data_splited')   #The model finds the number of PCA components that minimizes the validation data (20% of all data) and will be trained all 80% of the training data\n",
    "\n",
    "        else:\n",
    "            number_of_components_to_train_with = 24              #It's a number between 1-192\n",
    "            rn_skl.train_model_pca(num_comp=number_of_components_to_train_with)\n",
    "    \n",
    "    training_loss_features_pca = rn_skl.get_training_loss_pca_data()\n",
    "    print(\"training_loss_model_PCA: \",training_loss_features_pca)\n",
    "\n",
    "    if submit:\n",
    "        rn_skl.submit_test_results_pca()\n",
    "\n",
    "else:\n",
    "    print(\"ERROR, data_to_learn_with must be : 'features', 'images', 'features_images' or 'pca_features' \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
